#!/usr/bin/env python3

# 4chan downloader
# Copyright (C) 2012 Marian Beermann
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# Just pipe thread urls into this. subfolders are created automatically.

import urllib.request, urllib.error, urllib.parse
import sys
import io
import re
import string
import os
import os.path
from lxml import html

HEADERS = {
	# Accept must be set and User-Agent cannot be the standard one.
	"Accept": "text/html, application/xml;q=0.9, application/xhtml+xml, image/png, image/webp, image/jpeg, image/gif, image/x-xbitmap, */*;q=0.1",
	"User-Agent": "Opera/9.80 (X11; Linux x86_64) Presto/2.12.388 Version/12.16",
	#"Accept-Language": "en-US,en;q=0.9",
}

def sanitize_folder_name(name):
	return "".join(c for c in name if c in set(string.ascii_letters + string.digits + '-_.[]{}()$ '))

def httpreq(url):
	req = urllib.request.Request(url=url, headers=HEADERS)
	return urllib.request.urlopen(req)

if __name__ == "__main__":
	i = 0
	for url in sys.stdin:
		j = 0
		url = url.strip()
		if not url:
			continue

		tid = re.search(r"(\d{2,})", url).group(0)
		print("Downloading thread %s" % tid, end=' ')

		try:
			pagehtml = str(httpreq(url).read())
		except urllib.error.HTTPError as failure:
			print("Error fetching %s: %s" % (url, failure))
			continue
		
		page = html.fromstring(pagehtml)

		title = page.cssselect("#pi%s .subject" % tid)
		
		if not len(title[0].text_content()):
			title = ""
			folder = tid
		else:
			title = title[0].text_content()
			folder = sanitize_folder_name("%s - %s" % (tid, title))

		try:
			os.mkdir(folder)
		except os.error as ose:
			if ose.errno == 17:
				pass

		files = set(re.findall(r'(images\.4chan\.org/[^/]*/src/[^"<]*)', pagehtml))
		print("- %s files" % len(files))

		for fileurl in files:
			filename = os.path.basename(fileurl)
			filepath = "%s/%s" % (folder, filename)

			if not os.path.exists(filepath):
				try:
					response = httpreq("http://%s" % fileurl)

					total_size = int(response.info()['Content-Length'].strip())
					bytes_so_far = 0

					with open(filepath, "wb") as f:
						while True:
							chunk = response.read(8192)
							f.write(chunk)

							bytes_so_far += len(chunk)

							sys.stdout.write("%s: %d of %d bytes (%0.2f%%)\r" % (filename, bytes_so_far, total_size, round((float(bytes_so_far) / total_size)*100, 2)))

							if bytes_so_far >= total_size:
								sys.stdout.write(" "*70 + "\r")
								break
					i += 1
					j += 1
				except urllib.error.HTTPError as failure:
					print("Error downloading %s from %s: %s" % (filename, fileurl, failure))

		print("+%s" % j)
	print()
	print("+%s" % i)
